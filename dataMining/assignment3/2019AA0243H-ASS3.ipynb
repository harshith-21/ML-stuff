{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2019AA0243H-ASS3.ipynb","provenance":[],"authorship_tag":"ABX9TyNDSRevikLyKT/rCRlAtFYI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6nFTZKUINIX","executionInfo":{"status":"ok","timestamp":1649438482252,"user_tz":-330,"elapsed":27129,"user":{"displayName":"HARSHITH GANDHE","userId":"10725924555064938006"}},"outputId":"e31de2be-9d88-441f-a026-6fbff6f9cb95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","path= \"/content/drive/MyDrive/dataminassi/assignment3/cars.csv\""]},{"cell_type":"code","source":["class NaiveBayesClassifier():\n","    '''\n","    Bayes Theorem form\n","    P(y|X) = P(X|y) * P(y) / P(X)\n","    '''\n","    def calc_prior(self, features, target):\n","        '''\n","        prior probability P(y)\n","        calculate prior probabilities\n","        '''\n","        self.prior = (features.groupby(target).apply(lambda x: len(x)) / self.rows).to_numpy()\n","\n","        return self.prior\n","    \n","    def calc_statistics(self, features, target):\n","        '''\n","        calculate mean, variance for each column and convert to numpy array\n","        ''' \n","        self.mean = features.groupby(target).apply(np.mean).to_numpy()\n","        self.var = features.groupby(target).apply(np.var).to_numpy()\n","              \n","        return self.mean, self.var\n","    \n","    def gaussian_density(self, class_idx, x):     \n","        '''\n","        calculate probability from gaussian density function (normally distributed)\n","        we will assume that probability of specific target value given specific class is normally distributed \n","        \n","        probability density function derived from wikipedia:\n","        (1/√2pi*σ) * exp((-1/2)*((x-μ)^2)/(2*σ²)), where μ is mean, σ² is variance, σ is quare root of variance (standard deviation)\n","        '''\n","        mean = self.mean[class_idx]\n","        var = self.var[class_idx]\n","        numerator = np.exp((-1/2)*((x-mean)**2) / (2 * var))\n","#         numerator = np.exp(-((x-mean)**2 / (2 * var)))\n","        denominator = np.sqrt(2 * np.pi * var)\n","        prob = numerator / denominator\n","        return prob\n","    \n","    def calc_posterior(self, x):\n","        posteriors = []\n","\n","        # calculate posterior probability for each class\n","        for i in range(self.count):\n","            prior = np.log(self.prior[i]) ## use the log to make it more numerically stable\n","            conditional = np.sum(np.log(self.gaussian_density(i, x))) # use the log to make it more numerically stable\n","            posterior = prior + conditional\n","            posteriors.append(posterior)\n","        # return class with highest posterior probability\n","        return self.classes[np.argmax(posteriors)]\n","     \n","\n","    def fit(self, features, target):\n","        self.classes = np.unique(target)\n","        self.count = len(self.classes)\n","        self.feature_nums = features.shape[1]\n","        self.rows = features.shape[0]\n","        \n","        self.calc_statistics(features, target)\n","        self.calc_prior(features, target)\n","        \n","    def predict(self, features):\n","        preds = [self.calc_posterior(f) for f in features.to_numpy()]\n","        return preds\n","\n","    def accuracy(self, y_test, y_pred):\n","        accuracy = np.sum(y_test == y_pred) / len(y_test)\n","        return accuracy\n","\n","    def visualize(self, y_true, y_pred, target):\n","        \n","        tr = pd.DataFrame(data=y_true, columns=[target])\n","        pr = pd.DataFrame(data=y_pred, columns=[target])\n","        \n","        \n","        fig, ax = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(15,6))\n","        \n","        sns.countplot(x=target, data=tr, ax=ax[0], palette='viridis', alpha=0.7, hue=target, dodge=False)\n","        sns.countplot(x=target, data=pr, ax=ax[1], palette='viridis', alpha=0.7, hue=target, dodge=False)\n","        \n","\n","        fig.suptitle('True vs Predicted Comparison', fontsize=20)\n","\n","        ax[0].tick_params(labelsize=12)\n","        ax[1].tick_params(labelsize=12)\n","        ax[0].set_title(\"True values\", fontsize=18)\n","        ax[1].set_title(\"Predicted values\", fontsize=18)\n","        plt.show()"],"metadata":{"id":"A25uJwRoJDUX","executionInfo":{"status":"ok","timestamp":1649438497551,"user_tz":-330,"elapsed":483,"user":{"displayName":"HARSHITH GANDHE","userId":"10725924555064938006"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# pre-process dataset for training \n","\n","# upload Iris dataset -  shape is (150, 5)\n","df = pd.read_csv(\"/content/drive/MyDrive/dataminassi/assignment3/cars.csv\")\n","# shuffle dataset with sample\n","df = df.sample(frac=1, random_state=1).reset_index(drop=True)\n","# df shape\n","print(df.shape)\n","# set features and target\n","X, y = df.iloc[:, :-1], df.iloc[:, -1]\n","\n","\n","# # split on train and test 0.7/0.3\n","X_train, X_test, y_train, y_test = X[:100], X[100:], y[:100], y[100:]\n","\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRS4d9k2gE1G","executionInfo":{"status":"ok","timestamp":1649438514431,"user_tz":-330,"elapsed":639,"user":{"displayName":"HARSHITH GANDHE","userId":"10725924555064938006"}},"outputId":"18d3dac6-fdf9-4019-d957-7beef91cb502"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(1728, 7)\n","(100, 6) (100,)\n","(1628, 6) (1628,)\n"]}]},{"cell_type":"code","source":["# train the model\n","x = NaiveBayesClassifier()\n","\n","\n","x.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDgMEvBohnUr","executionInfo":{"status":"ok","timestamp":1649438522824,"user_tz":-330,"elapsed":436,"user":{"displayName":"HARSHITH GANDHE","userId":"10725924555064938006"}},"outputId":"4b5dfbe6-b372-4363-93d0-2ada95f168fb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3438: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n","  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3721: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n","  return var(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n"]}]},{"cell_type":"code","source":["# df\n","\n","# X_train.shape\n","\n","# x.classes, x.feature_nums, x.rows, x.count\n","# x.calc_prior(X_train, y_train)\n","# x.prior\n","x.calc_statistics(X_train, y_train)\n","\n","\n","# predictions = x.predict(X_test)\n","# x.accuracy(y_test, predictions)\n","# y_test.value_counts(normalize=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyZCYeuAhpdQ","executionInfo":{"status":"ok","timestamp":1649427914784,"user_tz":-330,"elapsed":11,"user":{"displayName":"HARSHITH GANDHE","userId":"10725924555064938006"}},"outputId":"cdb69261-fa46-4da0-b310-212065a8668b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3438: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n","  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3721: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n","  return var(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n"]},{"output_type":"execute_result","data":{"text/plain":["(array([], shape=(4, 0), dtype=float64),\n"," array([], shape=(4, 0), dtype=float64))"]},"metadata":{},"execution_count":24}]}]}
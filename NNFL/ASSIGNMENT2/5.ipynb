{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNfgZulmQClrCDQy5S0BcV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eIlCQA-R1P9h"},"source":["#5"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRwavA69FwpC","executionInfo":{"status":"ok","timestamp":1638369696356,"user_tz":-330,"elapsed":670,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"6bc51cd4-8534-4674-cbc6-fb53893e9e38"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"ULwv02O-F00P","executionInfo":{"status":"ok","timestamp":1638369696894,"user_tz":-330,"elapsed":10,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["%reset-f\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","#normalising the instances\n","def normal(X_tr,X_val,X_te):\n","  mean=X_tr.mean(axis=1)\n","  std=X_tr.std(axis=1)\n","  for i in range(X_tr.shape[0]):\n","    X_tr[i:i+1,:]=(X_tr[i:i+1,:]-mean[i])/std[i]\n","    X_val[i:i+1,:]=(X_val[i:i+1,:]-mean[i])/std[i]\n","    X_te[i:i+1,:]=(X_te[i:i+1,:]-mean[i])/std[i]\n","  return X_tr,X_val,X_te\n","\n","#splitting instances into training,testing and validation\n","def split_inst(inst,train_ratio,val_ratio):\n","  X=inst[:inst.shape[0]-1,:]\n","  Y=inst[inst.shape[0]-1:,:]\n","  X_tr=X[:,:int(train_ratio*X.shape[1])]\n","  Y_tr=Y[:,:int(train_ratio*Y.shape[1])]\n","  X_val=X[:,int(train_ratio*X.shape[1]):int((train_ratio+val_ratio)*X.shape[1])]\n","  Y_val=Y[:,int(train_ratio*Y.shape[1]):int((train_ratio+val_ratio)*Y.shape[1])]\n","  X_te=X[:,int((train_ratio+val_ratio)*X.shape[1]):]\n","  Y_te=Y[:,int((train_ratio+val_ratio)*Y.shape[1]):]\n","  return X_tr,X_val,X_te,Y_tr,Y_val,Y_te\n","\n","#Running all functions invloved in preprocessing of the instances\n","def preprocess(inst):\n","  inst=df.values\n","  inst=Randomize(inst)\n","  inst=inst.T\n","  X_tr,X_val,X_te,Y_tr,Y_val,Y_te=split_inst(inst,0.7,0.1)\n","  X_tr,X_val,X_te=normal(X_tr,X_val,X_te)\n","  Y_tr=hot_encoding(Y_tr,3)\n","  Y_val=hot_encoding(Y_val,3)\n","  Y_te=hot_encoding(Y_te,3)\n","  return X_tr,X_val,X_te,Y_tr,Y_val,Y_te\n","\n","#randomly shuffling the instances\n","def Randomize(inst):\n","  np.random.shuffle(inst)\n","  return inst\n","\n","def activation(Z,k):\n","  if k== 1:\n","    A=(np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n","  if k==2:\n","    A=1/(1+np.exp(-Z))\n","  return A\n","\n","def cost(a,b):\n","  return (1/a.shape[1])*np.sum((a-b)**2)\n","\n","def return_max(Y_hat):\n","  for i in range(Y_hat.shape[1]):\n","    if Y_hat[0][i]>Y_hat[1][i] and Y_hat[0][i]>Y_hat[2][i]:\n","      Y_hat[0][i]=1\n","      Y_hat[1][i]=0\n","      Y_hat[2][i]=0\n","    elif Y_hat[1][i]>Y_hat[0][i] and Y_hat[1][i]>Y_hat[2][i]:\n","      Y_hat[0][i]=0\n","      Y_hat[1][i]=1\n","      Y_hat[2][i]=0\n","    else:\n","      Y_hat[0][i]=0\n","      Y_hat[1][i]=0\n","      Y_hat[2][i]=1\n","  return Y_hat\n","\n","\n","def accuracy(Y_hat,Y):\n","  a=np.abs(Y_hat-Y)\n","  num_mistakes=(np.sum(a))/2\n","  acc=100*(Y.shape[1]-num_mistakes)/Y.shape[1]\n","  return acc\n","\n","def hot_encoding(Y,n_Y):\n","  Y_new=np.zeros((n_Y,Y.shape[1]))\n","  for i in range(Y.shape[1]):\n","    Y_new[int(Y[0,i])-1][i]=1\n","  return Y_new\n","\n","  "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSpE4xEQGrgD"},"source":["#STACKED AUTOENCODER"]},{"cell_type":"code","metadata":{"id":"Lsu4oIxAGqxm","executionInfo":{"status":"ok","timestamp":1638369697597,"user_tz":-330,"elapsed":711,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["def pretraining(input,n_h):\n","  W1=np.random.randn(n_h,input.shape[0])\n","  W2=np.random.randn(input.shape[0],n_h)\n","  b1=np.zeros((n_h,1))\n","  b2=np.zeros((input.shape[0],1))\n","  m=input.shape[1]\n","  for i in range(1000):\n","    #forward propagation\n","    Z1=np.dot(W1,input)+b1\n","    A=activation(Z1,1)\n","    Z2=np.dot(W2,A)+b2\n","    output=activation(Z2,1)\n","    #backpropagation\n","    dZ2=(output-input)*(1-output**2)\n","    dW2=(1/m)*np.dot(dZ2,A.T)\n","    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)\n","    dA=np.dot(W2.T,dZ2)\n","    dZ1=dA*(1-A**2)\n","    dW1=(1/m)*np.dot(dZ1,input.T)\n","    db1=(1/m)*np.sum(dZ1,axis=1,keepdims=True)\n","    #gradient descent\n","    W1=W1-0.5*dW1\n","    W2=W2-0.5*dW2\n","    b1=b1-0.5*db1\n","    b2=b2-0.5*db2\n","    #cost\n","  return W1,b1,A\n","\n","def forward_propagation(X,parameters):\n","  Z1=np.dot(parameters[\"W1\"],X)+parameters[\"b1\"]\n","  A1=activation(Z1,1)\n","  Z2=np.dot(parameters[\"W2\"],A1)+parameters[\"b2\"]\n","  A2=activation(Z2,1)\n","  Z3=np.dot(parameters[\"W3\"],A2)+parameters[\"b3\"]\n","  A3=activation(Z3,1)\n","  Z4=np.dot(parameters[\"W4\"],A3)+parameters[\"b4\"]\n","  Y_hat= activation(Z4,2)\n","  nodes={\"Z1\":Z1,\n","         \"A1\":A1,\n","         \"Z2\":Z2,\n","         \"A2\":A2,\n","         \"Z3\":Z3,\n","         \"A3\":A3,\n","         \"Z4\":Z4,\n","         \"Y_hat\":Y_hat}\n","  return nodes\n","\n","\n","#calculating backward propagation for a single iteration and returning the value of the gradient of each neuron\n","def back_propagation(X,Y,parameters,nodes):\n","  m=X.shape[1]\n","  dZ4=(nodes[\"Y_hat\"]-Y)*nodes[\"Y_hat\"]*(1-nodes[\"Y_hat\"]**2)\n","  dW4=(1/m)*np.dot(dZ4,nodes[\"A3\"].T)\n","  db4=(1/m)*np.sum(dZ4,axis=1,keepdims=True)\n","  dA3=np.dot(parameters[\"W4\"].T,dZ4)\n","  dZ3=dA3*(1-nodes[\"A3\"]**2)\n","  dW3=(1/m)*np.dot(dZ3,nodes[\"A2\"].T)\n","  db3=(1/m)*np.sum(dZ3,axis=1,keepdims=True)\n","  dA2=np.dot(parameters[\"W3\"].T,dZ3)\n","  dZ2=dA2*(1-nodes[\"A2\"]**2)\n","  dW2=(1/m)*np.dot(dZ2,nodes[\"A1\"].T)\n","  db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)\n","  dA1=np.dot(parameters[\"W2\"].T,dZ2)\n","  dZ1=dA1*(1-nodes[\"A1\"]**2)\n","  dW1=(1/m)*np.dot(dZ1,X.T)\n","  db1=(1/m)*np.sum(dZ1,axis=1,keepdims=True)\n","  grads={\"dW1\":dW1,\n","         \"db1\":db1,\n","         \"dW2\":dW2,\n","         \"db2\":db2,\n","         \"dW3\":dW3,\n","         \"db3\":db3,\n","         \"dW4\":dW4,\n","         \"db4\":db4}\n","  return grads\n","\n","#updating weights and biases \n","def gradient_descent(parameters,grads,learning_rate):\n","  parameters[\"W1\"]=parameters[\"W1\"]-learning_rate*grads[\"dW1\"]\n","  parameters[\"b1\"]=parameters[\"b1\"]-learning_rate*grads[\"db1\"]\n","  parameters[\"W2\"]=parameters[\"W2\"]-learning_rate*grads[\"dW2\"]\n","  parameters[\"b2\"]=parameters[\"b2\"]-learning_rate*grads[\"db2\"]\n","  parameters[\"W3\"]=parameters[\"W3\"]-learning_rate*grads[\"dW3\"]\n","  parameters[\"b3\"]=parameters[\"b3\"]-learning_rate*grads[\"db3\"]\n","  parameters[\"W4\"]=parameters[\"W4\"]-learning_rate*grads[\"dW4\"]\n","  parameters[\"b4\"]=parameters[\"b4\"]-learning_rate*grads[\"db4\"]\n","  return parameters\n","\n","\n","def finetuning(X,parameters,Y,learning_rate):\n","  for i in range(1000):\n","    nodes=forward_propagation(X,parameters)\n","    grads=back_propagation(X,Y,parameters,nodes)\n","    parameters=gradient_descent(parameters,grads,learning_rate)\n","    #if i%50==0:\n","      #print(\"Cost after iteration\", i, \":\", cost(nodes[\"Y_hat\"],Y))\n","  Y_hat=return_max(nodes[\"Y_hat\"])\n","  #print(\"training accuracy  :\", accuracy(Y_hat,Y), \"%\") \n","  return parameters,nodes\n","\n","\n","def model_predict(X,Y,parameters):\n","  nodes=forward_propagation(X,parameters)\n","  Y_hat=nodes[\"Y_hat\"]\n","  Y_hat=return_max(Y_hat)\n","  return accuracy(Y_hat,Y)\n","  "],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYbmgGWtGx5b"},"source":["##MAIN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9yQ4LquG0Rt","executionInfo":{"status":"ok","timestamp":1638369701576,"user_tz":-330,"elapsed":3981,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"22c0b93f-343d-4a45-8952-014df1aa5d83"},"source":["df=pd.read_excel(\"/content/drive/MyDrive/ML/data5.xlsx\")\n","\n","inst=df.values\n","np.random.seed(0)\n","X_tr,X_val,X_te,Y_tr,Y_val,Y_te=preprocess(inst)\n","W1,b1,A1=pretraining(X_tr,5)\n","W2,b2,A2=pretraining(A1,4)\n","W3,b3,A3=pretraining(A2,3)\n","W4=np.random.randn(3,3)\n","b4=np.zeros((3,1))\n","parameters={\"W1\":W1, \"b1\":b1,\"W2\":W2,\"b2\":b2,\"W3\":W3,\"b3\":b3,\"W4\":W4,\"b4\":b4}\n","max=0\n","alpha=np.arange(0.1,1.1,0.1)\n","acc=np.zeros(10)\n","for i in range(10):\n","  parameters,nodes=finetuning(X_tr,parameters,Y_tr,alpha[i])\n","  acc[i]=model_predict(X_val,Y_val,parameters)\n","  if acc[i]>max:\n","    max=acc[i]\n","    imax=i\n","    maxparameters=parameters\n","print(\"validation accuracy is\", acc[imax], \"%\")\n","acc=model_predict(X_te,Y_te,maxparameters)\n","print(\"testing accuracy is\", acc, \"%\") "],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["validation accuracy is 95.23809523809524 %\n","testing accuracy is 95.23809523809524 %\n"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0tY1HtlBjVzr"},"source":["### ***7***"]},{"cell_type":"markdown","metadata":{"id":"sW1-gpP6YYsK"},"source":["# Multiclass Logistic Regression + BGD + 5-Fold"]},{"cell_type":"code","metadata":{"id":"05F5V5tllUkm","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1634019996153,"user_tz":-330,"elapsed":28335,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"dc968615-861e-4207-b818-a7994d025d0a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"AE9px2PUx8P4","executionInfo":{"status":"ok","timestamp":1634020000718,"user_tz":-330,"elapsed":462,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["%reset-f"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuNZ9foax_y8","executionInfo":{"status":"ok","timestamp":1634020002677,"user_tz":-330,"elapsed":2,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt \n","import math\n","import copy"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDZn-s-5yC1d","executionInfo":{"status":"ok","timestamp":1634020005628,"user_tz":-330,"elapsed":1531,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["path = '/content/drive/MyDrive/ML/data_q6_q7.xlsx' \n","df = pd.read_excel(path)\n","insts = df.to_numpy()\n","m= len(insts[:,0]) #No of instinces\n","ones = np.ones((m,1))\n","insts = np.append(ones,insts,axis=1)\n","n = len(insts[0,:])-1 #no of features including f0 n=31\n","for i in range(1,n,1):\n","  insts[:,i] = (insts[:,i] - insts[:,i].mean())/insts[:,i].std() #Normalize the data\n","np.random.shuffle(insts)\n","insts_mb = copy.deepcopy(insts)\n","insts_s = copy.deepcopy(insts)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzXhUp3JyGf4","executionInfo":{"status":"ok","timestamp":1634020018930,"user_tz":-330,"elapsed":736,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["# hypothesis for logistic regression which is the prediction \n","def hypothesis(w,insts,m,n):\n","  z = np.dot(insts[:,0:n],w.T)\n","  h = 1/(1+np.exp(-z.astype(np.float32))) #working\n","  return h\n","\n","# cost function for logistic regression\n","def cost(h,insts,m):\n","  c = 0\n","  for i in range(m):\n","    c_te = (-insts_tr[i,-1]*np.log(h[i]) - (1- insts_tr[i,-1])*np.log(1 - h[i]))/m\n","    if not(np.isnan(c_te)) and not(math.isinf(c_te)):\n","      c = c+ c_te\n","  return c\n","# update of weight values for logistic regression\n","def update(w,alpha,insts,h,m,n):\n","  d = np.zeros(n)\n","  for i in range(n):\n","    d[i] = np.dot( (h[0:m]- insts[0:m,-1]),insts[0:m,i] )\n","  for i in range(n):\n","    w[i] = w[i] - alpha*d[i]\n","\n","  return w\n","def update_stotastic(w,alpha,insts,h,index,n):\n","  d = np.zeros(n)\n","  for i in range(n):\n","    d[i] = (h[index]- insts[index,-1])*insts[index,i] \n","  for i in range(n):\n","    w[i] = w[i] - alpha*d[i]\n","\n","  return w\n","\n","def performance(mat,m):\n","  p = []\n","  for i in range(3):\n","    ia = mat[i,i]/(mat[i,0]+mat[i,1]+mat[i,2])\n","    p.append(ia)\n","  \n","  p.append((mat[0,0]+mat[1,1]+mat[2,2])/m)\n","  return p\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fs0LyAeGySYw","executionInfo":{"status":"ok","timestamp":1633963015305,"user_tz":-330,"elapsed":38136,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"111df9bb-8f11-4511-ed42-4e28cf9d1393"},"source":["K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.0001,0.001,100)\n","    b_min = 10000\n","    c = np.ones(len(alpha))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    for j in range(len(alpha)):\n","      w = np.random.rand(n)\n","      j_list = np.ones(itr)\n","      for i in range(itr):\n","        h = hypothesis(w,train,m_tr,n)\n","        w = update(w,alpha[j],train,h,m_tr,n)\n","      h = hypothesis(w,train,m_tr,n)\n","      c[j] = cost(h,train,m_tr)\n","      if c[j] < b_min:\n","        min = j\n","        b_min = c[j]\n","        w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,m_tr)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","    \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_class1: 0.7319298245614035 \n","accuracy_class2: 0.9511111111111111 \n","accuracy_class3: 0.9549450549450551 \n","overall_accuracy: 0.870267131242741\n"]}]},{"cell_type":"markdown","metadata":{"id":"Tyc2xtOPpozw"},"source":["# Multiclass Logistic Regression + MBG + 5-Fold"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DsHEaLiupwQr","executionInfo":{"status":"ok","timestamp":1633963230451,"user_tz":-330,"elapsed":189449,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"6b0e3248-9823-4b66-9df8-48c3b4c13c12"},"source":["K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts_mb[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts_mb,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.001,0.01,100)\n","    b_min = 10000\n","    c = np.ones(len(alpha))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    m_batch =20\n","    for j in range(len(alpha)):\n","      w = np.random.rand(n)\n","      for i in range(itr):\n","        np.random.shuffle(train)\n","        h = hypothesis(w,train,m_tr,n)\n","        w = update(w,alpha[j],train,h,m_batch,n)\n","      h = hypothesis(w,train,m_tr,n)\n","      c[j] = cost(h,train,m_tr)\n","      if c[j] < b_min:\n","        min = j\n","        b_min = c[j]\n","        w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,m_tr)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  \n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","    \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_class1: 0.8276942355889723 \n","accuracy_class2: 0.9511111111111111 \n","accuracy_class3: 0.8673561732385263 \n","overall_accuracy: 0.8702671312427409\n"]}]},{"cell_type":"markdown","metadata":{"id":"3Oj9-hB03Gge"},"source":["# Multiclass Logistic Regression + SGD + 5-Fold"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQvA3C-R3Ovg","executionInfo":{"status":"ok","timestamp":1633963289938,"user_tz":-330,"elapsed":46772,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"5b588fa3-2190-4cb6-d4d0-0647ed3a12c6"},"source":["K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts_s[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts_s,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.01,0.1,100)\n","    b_min = 10000\n","    c = np.ones(len(alpha))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    for j in range(len(alpha)):\n","      w = np.random.rand(n)\n","      for i in range(itr):\n","        index = np.random.randint(m_tr)\n","        h = hypothesis(w,train,m_tr,n)\n","        w = update_stotastic(w,alpha[j],train,h,index,n)\n","      h = hypothesis(w,train,m_tr,n)\n","      c[j] = cost(h,train,m_tr)\n","      if c[j] < b_min:\n","        min = j\n","        b_min = c[j]\n","        w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,m_tr)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  \n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","    \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_class1: 0.7190225563909776 \n","accuracy_class2: 0.9622222222222222 \n","accuracy_class3: 0.9549450549450549 \n","overall_accuracy: 0.8702671312427409\n"]}]},{"cell_type":"markdown","metadata":{"id":"0SeUSn6A5O3J"},"source":["# Multiclass Logistic Regression + BGD + L2-NORM + 5-Fold"]},{"cell_type":"code","metadata":{"id":"fHSKhlD95Zz6","executionInfo":{"status":"ok","timestamp":1634020082987,"user_tz":-330,"elapsed":481,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["%reset-f"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoJ2pBY3-7wc","executionInfo":{"status":"ok","timestamp":1634020084735,"user_tz":-330,"elapsed":9,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt \n","import math\n","import copy\n","import warnings"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9EvzUBj-8Sn","executionInfo":{"status":"ok","timestamp":1634020087364,"user_tz":-330,"elapsed":465,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["path = '/content/drive/MyDrive/ML/data_q6_q7.xlsx' \n","df = pd.read_excel(path)\n","insts = df.to_numpy()\n","m= len(insts[:,0]) #No of instinces\n","ones = np.ones((m,1))\n","insts = np.append(ones,insts,axis=1)\n","n = len(insts[0,:])-1 #no of features including f0 n=31\n","for i in range(1,n,1):\n","  insts[:,i] = (insts[:,i] - insts[:,i].mean())/insts[:,i].std() #Normalize the data\n","np.random.shuffle(insts)\n","insts_mb = copy.deepcopy(insts)\n","insts_s = copy.deepcopy(insts)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXjfqzmz-8ne","executionInfo":{"status":"ok","timestamp":1634020089602,"user_tz":-330,"elapsed":4,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}}},"source":["# hypothesis for logistic regression which is the prediction \n","def hypothesis(w,insts,m,n):\n","  z = np.dot(insts[:,0:n],w.T)\n","  h = 1/(1+np.exp(-z.astype(np.float32))) #working\n","  return h\n","\n","# cost function for logistic regression\n","def cost(h,insts,lamb,m,w):\n","  c = 0\n","  for i in range(m):\n","    c_te = (-insts_tr[i,-1]*np.log(h[i]) - (1- insts_tr[i,-1])*np.log(1 - h[i]))/m\n","    if not(np.isnan(c_te)) and not(math.isinf(c_te)):\n","      c = c+ c_te\n","  return c + 0.5*lamb*(np.dot(w,w))\n","# update of weight values for logistic regression\n","def update(w,alpha,lamb,insts,h,m,n):\n","  d = np.zeros(n)\n","  for i in range(n):\n","    d[i] = np.dot( (h[0:m]- insts[0:m,-1]),insts[0:m,i] )\n","  for i in range(n):\n","    w[i] = w[i]*(1-alpha*lamb) - alpha*d[i]\n","\n","  return w\n","def update_stotastic(w,alpha,lamb,insts,h,index,n):\n","  d = np.zeros(n)\n","  for i in range(n):\n","    d[i] = (h[index]- insts[index,-1])*insts[index,i] \n","  for i in range(n):\n","    w[i] = w[i]*(1-alpha*lamb) - alpha*d[i]\n","\n","  return w\n","\n","def performance(mat,m):\n","  p = []\n","  for i in range(3):\n","    ia = mat[i,i]/(mat[i,0]+mat[i,1]+mat[i,2])\n","    p.append(ia)\n","  \n","  p.append((mat[0,0]+mat[1,1]+mat[2,2])/m)\n","  return p"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYIqLySV_y4a","executionInfo":{"status":"ok","timestamp":1633963383612,"user_tz":-330,"elapsed":41212,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"60ec894c-666b-4117-bdc9-35b3f66945ae"},"source":["K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.0001,0.001,10)\n","    lamb = np.linspace(0,100,10)\n","    b_min = 10000\n","    c = np.ones(len(lamb))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    for j in range(len(lamb)):\n","      for a in alpha:\n","        w = np.random.rand(n)\n","        j_list = np.ones(itr)\n","        for i in range(itr):\n","          h = hypothesis(w,train,m_tr,n)\n","          w = update(w,a,lamb[j],train,h,m_tr,n)\n","        h = hypothesis(w,train,m_tr,n)\n","        c[j] = cost(h,train,lamb[j],m_tr,w)\n","        if c[j] < b_min:\n","          min = j\n","          b_min = c[j]\n","          w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,lamb[min],m_tr,w_opt)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","    \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_class1: 0.7746430040547688 \n","accuracy_class2: 0.9221848739495797 \n","accuracy_class3: 0.9761403508771929 \n","overall_accuracy: 0.8852497096399536\n"]}]},{"cell_type":"markdown","metadata":{"id":"l7qzWIdmEIO0"},"source":["# Multiclass Logistic Regression + MBG + L2-NORM + 5-Fold"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9N9aFcfhEP8p","executionInfo":{"status":"ok","timestamp":1633963927405,"user_tz":-330,"elapsed":192851,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"7d974a14-277b-4304-ac34-b0610d0f1332"},"source":["K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts_mb[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts_mb,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.001,0.01,10)\n","    lamb = np.linspace(0,100,10)\n","    b_min = 10000\n","    c = np.ones(len(lamb))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    m_batch = 20\n","    for j in range(len(lamb)):\n","      for a in alpha:\n","        w = np.random.rand(n)\n","        j_list = np.ones(itr)\n","        for i in range(itr):\n","          np.random.shuffle(train)\n","          h = hypothesis(w,train,m_tr,n)\n","          w = update(w,a,lamb[j],train,h,m_batch,n)\n","        h = hypothesis(w,train,m_tr,n)\n","        c[j] = cost(h,train,lamb[j],m_tr,w)\n","        if c[j] < b_min:\n","          min = j\n","          b_min = c[j]\n","          w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,lamb[min],m_tr,w_opt)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","    \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  print(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[13  1  0]\n"," [ 0  9  0]\n"," [ 5  0 14]]\n","[[12  1  1]\n"," [ 0 15  0]\n"," [ 1  0 12]]\n","[[16  0  1]\n"," [ 6 11  0]\n"," [ 2  0  6]]\n","[[11  2  0]\n"," [ 0 14  0]\n"," [ 9  0  6]]\n","[[11  0  0]\n"," [ 2 13  0]\n"," [10  0  5]]\n","accuracy_class1: 0.9146089204912734 \n","accuracy_class2: 0.9027450980392157 \n","accuracy_class3: 0.6286504723346829 \n","overall_accuracy: 0.8033681765389081\n"]}]},{"cell_type":"markdown","metadata":{"id":"AgpH5UrVGjnq"},"source":["# Multiclass Logistic Regression + SGD + L2-NORM + 5-Fold"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"69C__CoqGsG_","executionInfo":{"status":"ok","timestamp":1634020207599,"user_tz":-330,"elapsed":56446,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"5eab98d7-63cf-42fa-bc5a-fdae637be5e3"},"source":["#warnings.filterwarnings('ignore')\n","K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts_mb[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts_mb,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.01,0.1,10)\n","    lamb = np.linspace(0,50,10)\n","    b_min = 10000\n","    c = np.ones(len(lamb))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    for j in range(len(lamb)):\n","      for a in alpha:\n","        w = np.random.rand(n)\n","        j_list = np.ones(itr)\n","        for i in range(itr):\n","          index = np.random.randint(m_tr)\n","          h = hypothesis(w,train,m_tr,n)\n","          w = update_stotastic(w,a,lamb[j],train,h,index,n)\n","        h = hypothesis(w,train,m_tr,n)\n","        c[j] = cost(h,train,lamb[j],m_tr,w)\n","        if c[j] < b_min:\n","          min = j\n","          b_min = c[j]\n","          w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,lamb[min],m_tr,w_opt)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    \n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","  \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["accuracy_class1: 0.6252350427350427 \n","accuracy_class2: 0.5872727272727273 \n","accuracy_class3: 0.9833333333333334 \n","overall_accuracy: 0.7369337979094077\n"]}]},{"cell_type":"markdown","metadata":{"id":"gP2ss_wiLVi1"},"source":["# Multiclass Logistic Regression + BGD + L1-NORM + 5-Fold"]},{"cell_type":"code","metadata":{"id":"ddx5wEOpLXk5"},"source":["%reset-f"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMuGy2GgLgS6"},"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt \n","import math\n","import copy\n","import warnings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spGhP8fkLkrM"},"source":["path = '/content/drive/MyDrive/ML/data_q6_q7.xlsx' \n","df = pd.read_excel(path)\n","insts = df.to_numpy()\n","m= len(insts[:,0]) #No of instinces\n","ones = np.ones((m,1))\n","insts = np.append(ones,insts,axis=1)\n","n = len(insts[0,:])-1 #no of features including f0 n=31\n","for i in range(1,n,1):\n","  insts[:,i] = (insts[:,i] - insts[:,i].mean())/insts[:,i].std() #Normalize the data\n","np.random.shuffle(insts)\n","insts_mb = copy.deepcopy(insts)\n","insts_s = copy.deepcopy(insts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50rqIfa5LlZI"},"source":["# hypothesis for logistic regression which is the prediction \n","def hypothesis(w,insts,m,n):\n","  z = np.dot(insts[:,0:n],w.T)\n","  h = 1/(1+np.exp(-z.astype(np.float32))) #working\n","  return h\n","\n","# cost function for logistic regression\n","def cost(h,insts,lamb,m,w):\n","  c = 0\n","  for i in range(m):\n","    c_te = (-insts_tr[i,-1]*np.log(h[i]) - (1- insts_tr[i,-1])*np.log(1 - h[i]))/m\n","    if not(np.isnan(c_te)) and not(math.isinf(c_te)):\n","      c = c+ c_te\n","  w_abs = np.absolute(w)\n","  w_abs = np.sum(w_abs)\n","  return c/m + 0.5*lamb*(w_abs)\n","# update of weight values for logistic regression\n","def update(w,alpha,lamb,insts,h,m,n):\n","  d = np.zeros(n)\n","  for i in range(n):\n","    d[i] = np.dot( (h[0:m]- insts[0:m,-1]),insts[0:m,i] )\n","  for i in range(n):\n","    w[i] = (w[i]-alpha*lamb*np.sign(w[i])) - alpha*d[i]\n","\n","  return w\n","def update_stotastic(w,alpha,lamb,insts,h,index,n):\n","  d = np.zeros(n)\n","  for i in range(n):\n","    d[i] = (h[index]- insts[index,-1])*insts[index,i] \n","  for i in range(n):\n","    w[i] = (w[i]-alpha*lamb*np.sign(w[i])) - alpha*d[i]\n","\n","  return w\n","\n","def performance(mat,m):\n","  p = []\n","  for i in range(3):\n","    ia = mat[i,i]/(mat[i,0]+mat[i,1]+mat[i,2])\n","    p.append(ia)\n","  \n","  p.append((mat[0,0]+mat[1,1]+mat[2,2])/m)\n","  return p\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"a3HAJbF2MFPE","executionInfo":{"status":"ok","timestamp":1634020333262,"user_tz":-330,"elapsed":45968,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"6e7b0d5e-7e40-4898-d203-988e90d71095"},"source":["K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.0001,0.001,10)\n","    lamb = np.linspace(0,100,10)\n","    b_min = 10000\n","    c = np.ones(len(lamb))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    for j in range(len(lamb)):\n","      for a in alpha:\n","        w = np.random.rand(n)\n","        j_list = np.ones(itr)\n","        for i in range(itr):\n","          h = hypothesis(w,train,m_tr,n)\n","          w = update(w,a,lamb[j],train,h,m_tr,n)\n","        h = hypothesis(w,train,m_tr,n)\n","        c[j] = cost(h,train,lamb[j],m_tr,w)\n","        if c[j] < b_min:\n","          min = j\n","          b_min = c[j]\n","          w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,lamb[min],m_tr,w_opt)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","    \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n","\n"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_class1: 0.6931837606837608 \n","accuracy_class2: 0.9566666666666667 \n","accuracy_class3: 0.9690476190476192 \n","overall_accuracy: 0.875609756097561\n"]}]},{"cell_type":"markdown","metadata":{"id":"XgENXze5Moph"},"source":["# Multiclass Logistic Regression + MBG + L1-NORM + 5-Fold"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"im7VbH8nMuMh","executionInfo":{"status":"ok","timestamp":1634020561559,"user_tz":-330,"elapsed":217529,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"e3c8ab20-ab04-48aa-95d2-b7776a28f3e1"},"source":["K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts_mb[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts_mb,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.001,0.01,10)\n","    lamb = np.linspace(0,100,10)\n","    b_min = 10000\n","    c = np.ones(len(lamb))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    m_batch = 20\n","    for j in range(len(lamb)):\n","      for a in alpha:\n","        w = np.random.rand(n)\n","        j_list = np.ones(itr)\n","        for i in range(itr):\n","          np.random.shuffle(train)\n","          h = hypothesis(w,train,m_tr,n)\n","          w = update(w,a,lamb[j],train,h,m_batch,n)\n","        h = hypothesis(w,train,m_tr,n)\n","        c[j] = cost(h,train,lamb[j],m_tr,w)\n","        if c[j] < b_min:\n","          min = j\n","          b_min = c[j]\n","          w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,lamb[min],m_tr,w_opt)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","    \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  print(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 7  1  2]\n"," [ 1 14  0]\n"," [ 3  0 14]]\n","[[12  0  0]\n"," [ 1 15  0]\n"," [ 3  0 11]]\n","[[16  0  0]\n"," [ 1 10  0]\n"," [ 2  0 13]]\n","[[15  1  2]\n"," [ 0 12  0]\n"," [ 4  0  8]]\n","[[11  0  2]\n"," [ 6 10  0]\n"," [ 1  0 11]]\n","accuracy_class1: 0.875897435897436 \n","accuracy_class2: 0.8809848484848484 \n","accuracy_class3: 0.8118487394957983 \n","overall_accuracy: 0.8560975609756099\n"]}]},{"cell_type":"markdown","metadata":{"id":"HExKZ8AsN1gc"},"source":["# Multiclass Logistic Regression + SGD + L1-NORM + 5-Fold"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"raNAvc-DN78w","executionInfo":{"status":"ok","timestamp":1634020803111,"user_tz":-330,"elapsed":56402,"user":{"displayName":"HARSHITH GANDHE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqqtCT1oqiyI5Eh9zv6fQFnG0ICZb1-YGmzniD-g=s64","userId":"10725924555064938006"}},"outputId":"1a71a7fe-155f-49a2-9b71-e6291f6ea780"},"source":["warnings.filterwarnings('ignore')\n","K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts_mb[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts_mb,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==l).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 500\n","    alpha = np.linspace(0.01,0.1,10)\n","    lamb = np.linspace(0,50,10)\n","    b_min = 10000\n","    c = np.ones(len(lamb))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    for j in range(len(lamb)):\n","      for a in alpha:\n","        w = np.random.rand(n)\n","        j_list = np.ones(itr)\n","        for i in range(itr):\n","          index = np.random.randint(m_tr)\n","          h = hypothesis(w,train,m_tr,n)\n","          w = update_stotastic(w,a,lamb[j],train,h,index,n)\n","        h = hypothesis(w,train,m_tr,n)\n","        c[j] = cost(h,train,lamb[j],m_tr,w)\n","        if c[j] < b_min:\n","          min = j\n","          b_min = c[j]\n","          w_opt = w\n","    h = hypothesis(w_opt,train,m_tr,n)\n","    opt_cost.append(cost(h,train,lamb[min],m_tr,w_opt)) # cal the final cost value\n","    opt_weights.append(w_opt) # append the optimal weight values for every group  \n","  m_te = len(insts_te[:,0]) \n","  y_acu = insts_te[:,-1].astype(int)\n","  y_pre = np.zeros(len(insts_te)).astype(int)\n","  for i in range(len(opt_weights)):\n","    h_te = hypothesis(opt_weights[i],insts_te,m_te,n)\n","    y_pe = np.round(h_te)\n","    \n","    for j in range(len(y_pe)):\n","      if y_pe[j] == 1 and y_pre[j] == 0:\n","        y_pre[j] = i+1\n","      if y_pe[j] == 1 and y_pre[j] !=0:\n","        if opt_cost[i] <= opt_cost[y_pre[j]-1]:\n","          y_pre[j] = i+1\n","      if y_pre[j] == 0 and i==2:\n","        y_pre[j] = 1\n","\n","  \n","  y_actual = pd.Series(y_acu, name='Actual')\n","  y_pred = pd.Series(y_pre, name='Predicted')\n","  confmat = pd.crosstab(y_actual, y_pred)\n","  confmat = np.asarray(confmat)\n","  p = performance(confmat,m_te)\n","  accuracy_class1 += p[0]\n","  accuracy_class2 += p[1]\n","  accuracy_class3 += p[2]\n","  overall_accuracy += p[3]\n","\n","print('accuracy_class1:',accuracy_class1/K,'\\naccuracy_class2:',accuracy_class2/K,'\\naccuracy_class3:',accuracy_class3/K,'\\noverall_accuracy:',overall_accuracy/K)\n"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_class1: 0.5414102564102564 \n","accuracy_class2: 0.7093939393939394 \n","accuracy_class3: 0.9857142857142858 \n","overall_accuracy: 0.7559814169570267\n"]}]}]}